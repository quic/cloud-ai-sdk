diff --git a/src/diffusers/__init__.py b/src/diffusers/__init__.py
index f480b410..39a06d7a 100644
--- a/src/diffusers/__init__.py
+++ b/src/diffusers/__init__.py
@@ -17,6 +17,7 @@ from .utils import (
     logging,
 )
 
+from .pipelines import AICRuntimeModel
 
 try:
     if not is_onnx_available():
diff --git a/src/diffusers/models/attention.py b/src/diffusers/models/attention.py
index b476e762..1ac2ec2c 100644
--- a/src/diffusers/models/attention.py
+++ b/src/diffusers/models/attention.py
@@ -21,6 +21,7 @@ from torch import nn
 from ..utils.import_utils import is_xformers_available
 from .cross_attention import CrossAttention
 from .embeddings import CombinedTimestepLabelEmbeddings
+from ..utils import GroupNormCustom, gelu_custom, silu_custom
 
 
 if is_xformers_available():
@@ -123,7 +124,8 @@ class AttentionBlock(nn.Module):
         batch, channel, height, width = hidden_states.shape
 
         # norm
-        hidden_states = self.group_norm(hidden_states)
+        #hidden_states = self.group_norm(hidden_states)
+        hidden_states = GroupNormCustom.apply(hidden_states, self.group_norm.num_channels, self.group_norm.num_groups, self.group_norm.weight, self.group_norm.bias, self.group_norm.eps)
 
         hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)
 
@@ -145,21 +147,35 @@ class AttentionBlock(nn.Module):
             )
             hidden_states = hidden_states.to(query_proj.dtype)
         else:
-            attention_scores = torch.baddbmm(
-                torch.empty(
-                    query_proj.shape[0],
-                    query_proj.shape[1],
-                    key_proj.shape[1],
-                    dtype=query_proj.dtype,
-                    device=query_proj.device,
-                ),
-                query_proj,
-                key_proj.transpose(-1, -2),
-                beta=0,
-                alpha=scale,
-            )
+            #attention_scores = torch.baddbmm(
+            #    torch.empty(
+            #        query_proj.shape[0],
+            #        query_proj.shape[1],
+            #        key_proj.shape[1],
+            #        dtype=query_proj.dtype,
+            #        device=query_proj.device,
+            #    ),
+            #    query_proj,
+            #    key_proj.transpose(-1, -2),
+            #    beta=0,
+            #    alpha=scale,
+            #)
+            #softmaxD32
+            key_proj = key_proj * scale
+            attention_scores = torch.bmm(query_proj, key_proj.transpose(-1,-2))
             attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)
             hidden_states = torch.bmm(attention_probs, value_proj)
+            #block_size = 64
+            #num_blocks = 64
+            #for i in range(num_blocks):
+            #    attention_weights = torch.bmm(query_proj[:,i*block_size:(i+1)*block_size,:], key_proj.transpose(-1,-2))
+            #    attention_weights = attention_weights*scale
+            #    attention_probs = torch.nn.functional.softmax(attention_weights, dim=-1)
+            #    hidden_states_ = torch.bmm(attention_probs, value_proj).float()
+            #    if i==0:
+            #        hidden_states = hidden_states_.clone().float()
+            #    else:
+            #        hidden_states = torch.cat([hidden_states, hidden_states_],dim=1).float()
 
         # reshape hidden_states
         hidden_states = self.reshape_batch_dim_to_heads(hidden_states)
diff --git a/src/diffusers/models/autoencoder_kl.py b/src/diffusers/models/autoencoder_kl.py
index 9cb0a4b2..ffcac7e6 100644
--- a/src/diffusers/models/autoencoder_kl.py
+++ b/src/diffusers/models/autoencoder_kl.py
@@ -157,6 +157,7 @@ class AutoencoderKL(ModelMixin, ConfigMixin):
 
         h = self.encoder(x)
         moments = self.quant_conv(h)
+        return moments
         posterior = DiagonalGaussianDistribution(moments)
 
         if not return_dict:
diff --git a/src/diffusers/models/cross_attention.py b/src/diffusers/models/cross_attention.py
index 9f994064..a2a3e7e0 100644
--- a/src/diffusers/models/cross_attention.py
+++ b/src/diffusers/models/cross_attention.py
@@ -17,7 +17,7 @@ import torch
 import torch.nn.functional as F
 from torch import nn
 
-from ..utils import deprecate, logging
+from ..utils import deprecate, logging, GroupNormCustom
 from ..utils.import_utils import is_xformers_available
 
 
@@ -102,7 +102,8 @@ class CrossAttention(nn.Module):
         # We use the AttnProcessor2_0 by default when torch2.x is used which uses
         # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention
         if processor is None:
-            processor = AttnProcessor2_0() if hasattr(F, "scaled_dot_product_attention") else CrossAttnProcessor()
+            #processor = AttnProcessor2_0() if hasattr(F, "scaled_dot_product_attention") else CrossAttnProcessor()
+            processor = CrossAttnProcessor()
         self.set_processor(processor)
 
     def set_use_memory_efficient_attention_xformers(
@@ -311,8 +312,20 @@ class CrossAttnProcessor:
         key = attn.head_to_batch_dim(key)
         value = attn.head_to_batch_dim(value)
 
-        attention_probs = attn.get_attention_scores(query, key, attention_mask)
-        hidden_states = torch.bmm(attention_probs, value)
+        #attention_probs = attn.get_attention_scores(query, key, attention_mask)
+        #hidden_states = torch.bmm(attention_probs, value)
+        block_size = min(128, hidden_states.shape[1])
+        num_blocks = max(1, hidden_states.shape[1]//block_size)
+        for i in range(num_blocks):
+            query_block = query[:,i*block_size:(i+1)*block_size,:]*attn.scale
+            attention_weights = torch.bmm(query_block, key.transpose(-1,-2))
+            #attention_weights = attention_weights*attn.scale
+            attention_probs = torch.nn.functional.softmax(attention_weights, dim=-1)
+            hidden_states_ = torch.bmm(attention_probs, value)
+            if i==0:
+                hidden_states = hidden_states_.clone()
+            else:
+                hidden_states = torch.cat([hidden_states, hidden_states_.clone()],dim=1)
         hidden_states = attn.batch_to_head_dim(hidden_states)
 
         # linear proj
@@ -397,7 +410,10 @@ class CrossAttnAddedKVProcessor:
 
         attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
 
-        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
+        #hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
+        hidden_states = GroupNormCustom.apply(hidden_states, attn.group_norm.num_channels, 
+                                              attn.group_norm.num_groups, attn.group_norm.weight, 
+                                              attn.group_norm.bias, attn.group_norm.eps)
 
         query = attn.to_q(hidden_states)
         query = attn.head_to_batch_dim(query)
diff --git a/src/diffusers/models/resnet.py b/src/diffusers/models/resnet.py
index 7c14a7c4..2274b5db 100644
--- a/src/diffusers/models/resnet.py
+++ b/src/diffusers/models/resnet.py
@@ -6,6 +6,7 @@ import torch.nn as nn
 import torch.nn.functional as F
 
 from .attention import AdaGroupNorm
+from ..utils import GroupNormCustom, gelu_custom, silu_custom
 
 
 class Upsample1D(nn.Module):
@@ -537,7 +538,8 @@ class ResnetBlock2D(nn.Module):
         if self.time_embedding_norm == "ada_group":
             hidden_states = self.norm1(hidden_states, temb)
         else:
-            hidden_states = self.norm1(hidden_states)
+            #hidden_states = self.norm1(hidden_states)
+            hidden_states = GroupNormCustom.apply(hidden_states, self.norm1.num_channels, self.norm1.num_groups, self.norm1.weight, self.norm1.bias, self.norm1.eps)
 
         hidden_states = self.nonlinearity(hidden_states)
 
diff --git a/src/diffusers/models/transformer_2d.py b/src/diffusers/models/transformer_2d.py
index 2515c54b..93b00ff6 100644
--- a/src/diffusers/models/transformer_2d.py
+++ b/src/diffusers/models/transformer_2d.py
@@ -20,7 +20,7 @@ from torch import nn
 
 from ..configuration_utils import ConfigMixin, register_to_config
 from ..models.embeddings import ImagePositionalEmbeddings
-from ..utils import BaseOutput, deprecate
+from ..utils import BaseOutput, deprecate, GroupNormCustom
 from .attention import BasicTransformerBlock
 from .embeddings import PatchEmbed
 from .modeling_utils import ModelMixin
@@ -246,7 +246,8 @@ class Transformer2DModel(ModelMixin, ConfigMixin):
             batch, _, height, width = hidden_states.shape
             residual = hidden_states
 
-            hidden_states = self.norm(hidden_states)
+            #hidden_states = self.norm(hidden_states)
+            hidden_states = GroupNormCustom.apply(hidden_states, self.norm.num_channels, self.norm.num_groups, self.norm.weight, self.norm.bias, self.norm.eps)
             if not self.use_linear_projection:
                 hidden_states = self.proj_in(hidden_states)
                 inner_dim = hidden_states.shape[1]
diff --git a/src/diffusers/models/unet_2d_blocks.py b/src/diffusers/models/unet_2d_blocks.py
index 8269b77f..4c21bb96 100644
--- a/src/diffusers/models/unet_2d_blocks.py
+++ b/src/diffusers/models/unet_2d_blocks.py
@@ -22,6 +22,7 @@ from .cross_attention import CrossAttention, CrossAttnAddedKVProcessor
 from .dual_transformer_2d import DualTransformer2DModel
 from .resnet import Downsample2D, FirDownsample2D, FirUpsample2D, KDownsample2D, KUpsample2D, ResnetBlock2D, Upsample2D
 from .transformer_2d import Transformer2DModel
+from ..utils import GroupNormCustom, silu_custom
 
 
 def get_down_block(
@@ -2115,7 +2116,8 @@ class AttnSkipUpBlock2D(nn.Module):
             skip_sample = 0
 
         if self.resnet_up is not None:
-            skip_sample_states = self.skip_norm(hidden_states)
+            #skip_sample_states = self.skip_norm(hidden_states)
+            skip_sample_states = GroupNormCustom.apply(hidden_states, self.skip_norm.num_channels, self.skip_norm.num_groups, self.skip_norm.weight, self.skip_norm.bias, self.skip_norm.eps)
             skip_sample_states = self.act(skip_sample_states)
             skip_sample_states = self.skip_conv(skip_sample_states)
 
@@ -2210,7 +2212,8 @@ class SkipUpBlock2D(nn.Module):
             skip_sample = 0
 
         if self.resnet_up is not None:
-            skip_sample_states = self.skip_norm(hidden_states)
+            #skip_sample_states = self.skip_norm(hidden_states)
+            skip_sample_states = GroupNormCustom.apply(hidden_states, self.skip_norm.num_channels, self.skip_norm.num_groups, self.skip_norm.weight, self.skip_norm.bias, self.skip_norm.eps)
             skip_sample_states = self.act(skip_sample_states)
             skip_sample_states = self.skip_conv(skip_sample_states)
 
diff --git a/src/diffusers/models/unet_2d_condition.py b/src/diffusers/models/unet_2d_condition.py
index 8309312b..146a2cc1 100644
--- a/src/diffusers/models/unet_2d_condition.py
+++ b/src/diffusers/models/unet_2d_condition.py
@@ -20,7 +20,7 @@ import torch.utils.checkpoint
 
 from ..configuration_utils import ConfigMixin, register_to_config
 from ..loaders import UNet2DConditionLoadersMixin
-from ..utils import BaseOutput, logging
+from ..utils import BaseOutput, logging, GroupNormCustom, silu_custom, gelu_custom
 from .cross_attention import AttnProcessor
 from .embeddings import GaussianFourierProjection, TimestepEmbedding, Timesteps
 from .modeling_utils import ModelMixin
@@ -645,7 +645,8 @@ class UNet2DConditionModel(ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin)
 
         # 6. post-process
         if self.conv_norm_out:
-            sample = self.conv_norm_out(sample)
+            #sample = self.conv_norm_out(sample)
+            sample = GroupNormCustom.apply(sample, self.conv_norm_out.num_channels, self.conv_norm_out.num_groups, self.conv_norm_out.weight, self.conv_norm_out.bias, self.conv_norm_out.eps)
             sample = self.conv_act(sample)
         sample = self.conv_out(sample)
 
diff --git a/src/diffusers/models/vae.py b/src/diffusers/models/vae.py
index c5142a8f..b4c65dc1 100644
--- a/src/diffusers/models/vae.py
+++ b/src/diffusers/models/vae.py
@@ -18,7 +18,7 @@ import numpy as np
 import torch
 import torch.nn as nn
 
-from ..utils import BaseOutput, randn_tensor
+from ..utils import BaseOutput, randn_tensor, GroupNormCustom, silu_custom, gelu_custom
 from .unet_2d_blocks import UNetMidBlock2D, get_down_block, get_up_block
 
 
@@ -108,7 +108,8 @@ class Encoder(nn.Module):
         sample = self.mid_block(sample)
 
         # post-process
-        sample = self.conv_norm_out(sample)
+        #sample = self.conv_norm_out(sample)
+        sample = GroupNormCustom.apply(sample, self.conv_norm_out.num_channels, self.conv_norm_out.num_groups, self.conv_norm_out.weight, self.conv_norm_out.bias, self.conv_norm_out.eps)
         sample = self.conv_act(sample)
         sample = self.conv_out(sample)
 
@@ -188,7 +189,8 @@ class Decoder(nn.Module):
             sample = up_block(sample)
 
         # post-process
-        sample = self.conv_norm_out(sample)
+        #sample = self.conv_norm_out(sample)
+        sample = GroupNormCustom.apply(sample, self.conv_norm_out.num_channels, self.conv_norm_out.num_groups, self.conv_norm_out.weight, self.conv_norm_out.bias, self.conv_norm_out.eps)
         sample = self.conv_act(sample)
         sample = self.conv_out(sample)
 
diff --git a/src/diffusers/pipelines/__init__.py b/src/diffusers/pipelines/__init__.py
index 5b6c729f..702acb68 100644
--- a/src/diffusers/pipelines/__init__.py
+++ b/src/diffusers/pipelines/__init__.py
@@ -8,6 +8,7 @@ from ..utils import (
     is_transformers_available,
 )
 
+from .aic_utils import AICRuntimeModel
 
 try:
     if not is_torch_available():
diff --git a/src/diffusers/pipelines/aic_utils.py b/src/diffusers/pipelines/aic_utils.py
new file mode 100644
index 00000000..dc4a7fb5
--- /dev/null
+++ b/src/diffusers/pipelines/aic_utils.py
@@ -0,0 +1,154 @@
+##############################################################################
+#
+# Copyright (c) 2019-2022 Qualcomm Technologies, Inc.
+# All Rights Reserved.
+# Confidential and Proprietary - Qualcomm Technologies, Inc.
+#
+# All data and information contained in or disclosed by this document are
+# confidential and proprietary information of Qualcomm Technologies, Inc., and
+# all rights therein are expressly reserved. By accepting this material, the
+# recipient agrees that this material and the information contained therein
+# are held in confidence and in trust and will not be used, copied, reproduced
+# in whole or in part, nor its contents revealed in any manner to others
+# without the express written permission of Qualcomm Technologies, Inc.
+#
+##############################################################################
+
+import os
+import sys
+from typing import List
+
+import numpy as np
+
+import platform
+sys.path.append(f"/opt/qti-aic/dev/lib/{platform.machine()}/")
+sys.path.append('/opt/qti-aic/dev/python/')
+
+import QAicApi_pb2 as aicapi
+
+
+from qaicrt import (
+    BufferIoTypeEnum,
+    Context,
+    ExecObj,
+    Program,
+    QAicProgramProperties,
+    QBuffer,
+    QIDList,
+    Qpc,
+    QStatus,
+)
+
+
+class AICRuntimeModel:
+    def from_pretrained(binary_path, **kwargs):
+        # kwargs would include following keys:
+        # device_id, binary_path, output_shape_dict, submit_retry_count,
+        # submit_timeout
+        self = AICRuntimeModel()
+        self.binary_path = binary_path
+        self.device_id = kwargs["device_id"]
+        model_name = kwargs['model_name']
+        self.model_name = model_name
+        self.context = kwargs['context']
+        self.program_group = kwargs['program_group']
+        #self.binary_path = kwargs["binary_path"]
+        if not os.path.isdir(self.binary_path):
+            print("Binary directory not found.")
+            exit()
+
+        if not os.path.isfile(self.binary_path + "/programqpc.bin"):
+            print("programqpc.bin not found at given binary path.")
+            exit()
+
+        self.dev_list = QIDList()
+        self.dev_list.append(self.device_id)
+        self.context = kwargs['context'] #Context(self.dev_list)
+        self.qpc = Qpc(self.binary_path)
+        #Get Output shapes from qpc
+        status, qdata = self.qpc.getIoDescriptor()
+        self.iodesc = aicapi.IoDesc()
+        self.iodesc.ParseFromString(qdata)
+        self.bindings = self.iodesc.io_sets[0].bindings
+        self.outputs_shape_dict = {}
+        self.outputs_dtype_dict = {}
+        np_type_dict = {0: np.float32,
+                        1: np.float16,
+                        2: np.int8,
+                        3: np.uint8,
+                        4: np.int16,
+                        5: np.int32,
+                        6: np.int32, 
+                        7: np.int64,
+                        8: bool}
+        for binding in self.bindings:
+            if binding.dir == 1:
+                self.outputs_shape_dict[binding.name] = np.array(binding.dims)
+                self.outputs_dtype_dict[binding.name] = np_type_dict[binding.type]
+
+        if self.qpc == None:
+            print("Unable to open program container: " + self.binary_path)
+            sys.exit(1)
+
+        self.buf_mappings = self.qpc.getBufferMappings()
+        self.properties = QAicProgramProperties()
+        self.properties.SubmitNumRetries = kwargs["submit_retry_count"]
+        self.properties.SubmitRetryTimeoutMs = kwargs["submit_timeout"]
+        #self.program = Program(self.context, self.dev_list[0], self.qpc, self.properties)
+        #print(self.program_group)
+        status, self.program = self.program_group.addProgram(self.qpc)
+        #self.execObj = ExecObj(self.context, self.program)
+        print(self.model_name,'is loaded')
+        return self
+    def __call__(self, **inputs):
+        outputs = self.run(list(inputs.values()))
+        return outputs
+
+    def run(self, input_data: List[np.array]):
+        # input_data : contains list of inputs in a sequence expected by model
+        if not hasattr(self,'execObj'):
+            try:
+                self.execObj = ExecObj(self.context, self.program)
+            except Exception as e:
+                print(e)
+        raw_buffers = []
+        for inp_data in input_data:
+            raw_buffers.append(inp_data.tobytes())
+
+        for mapping in self.buf_mappings:
+            if mapping.ioType == BufferIoTypeEnum.BUFFER_IO_TYPE_OUTPUT:
+                raw_buffers.append(bytearray(mapping.size))
+
+        qbuf_list = []
+        for buffer in raw_buffers:
+            qbuf_list.append(QBuffer(buffer))
+
+        self.execObj.setData(qbuf_list)
+        #self.execObj.run(qbuf_list)
+        self.program_group.enqueue(self.execObj)
+        self.execObj.waitForCompletion()
+        status, execObjData = self.execObj.getData()
+
+        if status != QStatus.QS_SUCCESS:
+            print("Error in QAic execution. Exiting....")
+            exit()
+
+        num_inputs = len(input_data)
+        num_outputs = len(self.outputs_shape_dict)
+        model_output = []
+
+        for idx, (data, (output_name, output_shape)) in enumerate(zip(
+            execObjData[num_inputs : num_inputs + num_outputs],
+            self.outputs_shape_dict.items(),
+        )):
+            output_buffer = bytearray(data)
+            #if self.model_name=='safety_checker' and idx==1:
+            #    output_data = np.frombuffer(output_buffer, dtype=(bool))
+            #else:
+            output_data = np.frombuffer(output_buffer, dtype=self.outputs_dtype_dict[output_name])
+            output_data = output_data.reshape(
+                output_shape
+            )
+            model_output.append(output_data)
+
+        return model_output
diff --git a/src/diffusers/pipelines/pipeline_utils.py b/src/diffusers/pipelines/pipeline_utils.py
index 65b348d2..c95d7b60 100644
--- a/src/diffusers/pipelines/pipeline_utils.py
+++ b/src/diffusers/pipelines/pipeline_utils.py
@@ -86,6 +86,7 @@ LOADABLE_CLASSES = {
         "SchedulerMixin": ["save_pretrained", "from_pretrained"],
         "DiffusionPipeline": ["save_pretrained", "from_pretrained"],
         "OnnxRuntimeModel": ["save_pretrained", "from_pretrained"],
+        "AICRuntimeModel": ["save_pretrained", "from_pretrained"],
     },
     "transformers": {
         "PreTrainedTokenizer": ["save_pretrained", "from_pretrained"],
@@ -897,6 +898,13 @@ class DiffusionPipeline(ConfigMixin):
                 if issubclass(class_obj, diffusers.OnnxRuntimeModel):
                     loading_kwargs["provider"] = provider
                     loading_kwargs["sess_options"] = sess_options
+                if issubclass(class_obj, diffusers.AICRuntimeModel):
+                    loading_kwargs['model_name'] = name
+                    loading_kwargs['device_id'] = device_map
+                    loading_kwargs['submit_timeout'] = 5000
+                    loading_kwargs['submit_retry_count'] = 5
+                    loading_kwargs['program_group'] = kwargs['program_group']
+                    loading_kwargs['context'] = kwargs['context']
 
                 is_diffusers_model = issubclass(class_obj, diffusers.ModelMixin)
 
diff --git a/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py b/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py
index 55b996e5..3902495b 100644
--- a/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py
+++ b/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py
@@ -204,6 +204,7 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
         return_dict: bool = True,
         callback: Optional[Callable[[int, int, np.ndarray], None]] = None,
         callback_steps: int = 1,
+        safety_checker_flag: bool = False,
     ):
         if isinstance(prompt, str):
             batch_size = 1
@@ -257,10 +258,14 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
         if accepts_eta:
             extra_step_kwargs["eta"] = eta
 
-        timestep_dtype = next(
-            (input.type for input in self.unet.model.get_inputs() if input.name == "timestep"), "tensor(float)"
-        )
-        timestep_dtype = ORT_TO_NP_TYPE[timestep_dtype]
+        #timestep_dtype = next(
+        #    (input.type for input in self.unet.model.get_inputs() if input.name == "timestep"), "tensor(float)"
+        #)
+        #timestep_dtype = ORT_TO_NP_TYPE[timestep_dtype]
+        timestep_dtype = np.int64
+        import time
+        unet_time = 0
+        sched_time = 0
 
         for i, t in enumerate(self.progress_bar(self.scheduler.timesteps)):
             # expand the latents if we are doing classifier free guidance
@@ -269,8 +274,11 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
             latent_model_input = latent_model_input.cpu().numpy()
 
             # predict the noise residual
-            timestep = np.array([t], dtype=timestep_dtype)
+            #timestep = np.array([t], dtype=timestep_dtype)
+            timestep = np.array([t]*2*latents.shape[0] if do_classifier_free_guidance else [i]*latents.shape[0], dtype=timestep_dtype)
+            curr_unet_time = time.time()
             noise_pred = self.unet(sample=latent_model_input, timestep=timestep, encoder_hidden_states=prompt_embeds)
+            unet_time += (time.time()-curr_unet_time)
             noise_pred = noise_pred[0]
 
             # perform guidance
@@ -279,10 +287,12 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
                 noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
 
             # compute the previous noisy sample x_t -> x_t-1
+            curr_sched_time = time.time()
             scheduler_output = self.scheduler.step(
                 torch.from_numpy(noise_pred), t, torch.from_numpy(latents), **extra_step_kwargs
             )
             latents = scheduler_output.prev_sample.numpy()
+            sched_time += (time.time()-curr_sched_time)
 
             # call the callback, if provided
             if callback is not None and i % callback_steps == 0:
@@ -291,14 +301,20 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
         latents = 1 / 0.18215 * latents
         # image = self.vae_decoder(latent_sample=latents)[0]
         # it seems likes there is a strange result for using half-precision vae decoder if batchsize>1
-        image = np.concatenate(
-            [self.vae_decoder(latent_sample=latents[i : i + 1])[0] for i in range(latents.shape[0])]
-        )
+        #image = np.concatenate(
+        #    [self.vae_decoder(latent_sample=latents[i : i + 1])[0] for i in range(latents.shape[0])]
+        #)
+        vae_decoder_time = time.time()
+        image = self.vae_decoder(latent_sample=latents)[0]
+        vae_decoder_time = time.time()-vae_decoder_time
+        print('20 runs unet time',unet_time)
+        print('20 runs scheduler time',sched_time)
+        print('1 run vae time',vae_decoder_time)
 
         image = np.clip(image / 2 + 0.5, 0, 1)
         image = image.transpose((0, 2, 3, 1))
 
-        if self.safety_checker is not None:
+        if self.safety_checker is not None and safety_checker_flag:
             safety_checker_input = self.feature_extractor(
                 self.numpy_to_pil(image), return_tensors="np"
             ).pixel_values.astype(image.dtype)
@@ -308,8 +324,12 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
                 image_i, has_nsfw_concept_i = self.safety_checker(
                     clip_input=safety_checker_input[i : i + 1], images=image[i : i + 1]
                 )
-                images.append(image_i)
-                has_nsfw_concept.append(has_nsfw_concept_i[0])
+                if image_i.dtype==np.float32:
+                    images.append(image_i)
+                    has_nsfw_concept.append(has_nsfw_concept_i[0])
+                else:
+                    images.append(has_nsfw_concept_i)
+                    has_nsfw_concept.append(image_i[0])
             image = np.concatenate(images)
         else:
             has_nsfw_concept = None
diff --git a/src/diffusers/utils/__init__.py b/src/diffusers/utils/__init__.py
index 64d5c695..a5bdec57 100644
--- a/src/diffusers/utils/__init__.py
+++ b/src/diffusers/utils/__init__.py
@@ -69,7 +69,7 @@ from .import_utils import (
 from .logging import get_logger
 from .outputs import BaseOutput
 from .pil_utils import PIL_INTERPOLATION
-from .torch_utils import randn_tensor
+from .torch_utils import randn_tensor, GroupNormCustom, silu_custom, gelu_custom
 
 
 if is_torch_available():
diff --git a/src/diffusers/utils/torch_utils.py b/src/diffusers/utils/torch_utils.py
index 113e64c1..230177ed 100644
--- a/src/diffusers/utils/torch_utils.py
+++ b/src/diffusers/utils/torch_utils.py
@@ -22,9 +22,88 @@ from .import_utils import is_torch_available
 
 if is_torch_available():
     import torch
+    from torch import nn
 
 logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
 
+def gelu_custom(x):
+    return x*(x*(0.14670403 + 0.001198*x)+0.500781)
+    #return nn.GELU()(x)
+
+def silu_custom(x):
+    return x*(x*(0.2496902591 + 0.0034703712*x)+0.500781)
+    #return nn.SiLU()(x)
+
+class GroupNormCustom(torch.autograd.Function):
+    @staticmethod
+    def forward(self, data, num_channels, num_groups, weight, bias, eps):
+        gn = nn.GroupNorm(num_channels=num_channels, num_groups=num_groups)
+        gn.weight = weight
+        gn.bias = bias
+        gn.eps = eps
+        x = gn(data)
+        return x
+
+    @staticmethod
+    def symbolic(g, data, num_channels, num_groups, weight, bias, eps):
+        from torch.onnx import _constants, _deprecation, _type_utils, errors, symbolic_helper
+        from torch.onnx.symbolic_opset9 import add,mul
+        channel_size = symbolic_helper._get_tensor_dim_size(data, 1)
+        if channel_size is not None:
+            assert channel_size % num_groups == 0
+        input_rank = symbolic_helper._get_tensor_rank(data)
+        if input_rank is None:
+            return symbolic_helper._unimplemented("group_norm", "unknown input rank", data)
+        input_reshaped1 = symbolic_helper._unsqueeze_helper(g, data, [2])
+        # 0 in the shape list keeps dimension value unchanged.
+        shape = [0, num_groups, -1, 0, 0]
+        input_reshaped2 = symbolic_helper._reshape_helper(
+            g, input_reshaped1, g.op("Constant", value_t=torch.LongTensor(shape))
+        )
+        input_reshaped3 = symbolic_helper._flatten_helper(g, input_reshaped2, 0, 1, 5)
+        num_groups = 1
+        shape = [0, num_groups, -1]
+        input_reshaped = symbolic_helper._reshape_helper(
+            g, input_reshaped3, g.op("Constant", value_t=torch.LongTensor(shape))
+        )
+        weight_ = g.op(
+            "Constant",
+            value_t=torch.tensor(
+                [1.0] * num_groups,
+                dtype=_type_utils.JitScalarType.from_value(data).dtype(),
+            ),
+        )
+        bias_ = g.op(
+            "Constant",
+            value_t=torch.tensor(
+                [0.0] * num_groups,
+                dtype=_type_utils.JitScalarType.from_value(data).dtype(),
+            ),
+        )
+
+        norm_reshaped = g.op(
+            "InstanceNormalization", input_reshaped, weight_, bias_, epsilon_f=eps
+        )
+        norm = symbolic_helper._reshape_helper(g, norm_reshaped, g.op("Shape", data))
+
+        if weight is None or weight.node().mustBeNone():
+            weight_value = torch.tensor(
+                [1.0], dtype=_type_utils.JitScalarType.from_value(data).dtype()
+            )
+            weight = g.op("Constant", value_t=weight_value)
+        if bias is None or bias.node().mustBeNone():
+            bias_value = torch.tensor(
+                [0.0], dtype=_type_utils.JitScalarType.from_value(data).dtype()
+            )
+            bias = g.op("Constant", value_t=bias_value)
+
+        # Norm has shape [N, C, *] so we reshape weight and bias to [C, *]
+        axes = list(range(1, input_rank - 1))
+        return add(
+            g,
+            mul(g, norm, symbolic_helper._unsqueeze_helper(g, weight, axes)),
+            symbolic_helper._unsqueeze_helper(g, bias, axes),
+        )
 
 def randn_tensor(
     shape: Union[Tuple, List],
